// 整体框架逻辑：基于大模型文本生成API，接收用户输入的文本和生成长度参数，返回模型生成的文本结果
// 1.初始化
// 2.API处理流程 用户--API--模型--API--用户
// 3.文本生成控制

// 核心模块：
// 1.FastAPI：提供web服务框架，处理HTTP请求和响应
// 2.Pydantic：验证客户端请求数据
// 3.Hugging Face Transformers：用于加载预训练大语言模型进行文本生成
// 4.Uvicorn:本地开发时的web服务器，用于运行API启动服务


from fastapi import FastAPI, HTTPException #用于创建API的核心框架，处理HTTP异常
from pydantic import BaseModel #pydantic深度学习框架
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification #hugging face 的transformers库,自动加载训练模型和分词器


app = FastAPI()#fastAPI初始化应用
# Load model and tokenizer
model_path = "/data2/ygong/code/Qwen3-0.6B" #指定模型位置
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True) #加载分词器
model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True) #加载模型本体
model.eval() #将pytorch切换到推理模式
class TextRequest(BaseModel): #继承自pydantic的BaseModel类，定义输入的文本
    text: str #必填字段，必须是字符串类型
    max_length: int = 50 #可选字段，默认为整数，防止生成过长的文本
@app.post("/generate")#定义post接口，路径为/generate，用于接收客户端需求
async def generate_text(request: TextRequest): #使用之前定义的TextRequest类作为参数
    input_ids = tokenizer.encode(request.text, return_tensors="pt") #将用户输入的文本转换为模型可以理解的token ID 序列
    
    output = model.generate(
        input_ids,#编码后的输入文本token IDs 
        max_length=request.max_length, #生成结果的最大长度
        num_return_sequences=1, #生成几条候选结果
        no_repeat_ngram_size=2,#禁止重复的2-gram
        do_sample=True,#启用随机采样，而非贪婪搜索，生成结果多样化
        top_k=50,#仅从概率最高的50个token中采样
        top_p=0.95,#核采样，保留概率最高的95%的token
        temperature=0.7#控制随机性，值越低，结果越确定；值越高，结果越随机。0.7是一个中等值，适合大多数创意文本生成；1.0完全按模型概率采样，0.1趋向确定性输出
    )
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)#将模型生成的tokenID序列转换回人类可读的文本，并通过API返回给用户
    return {"generated_text": generated_text}
·
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

# FastAPI应用的本地启动脚本，用于快速运行一个开发服务器